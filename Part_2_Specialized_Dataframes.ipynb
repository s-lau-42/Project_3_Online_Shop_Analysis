{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "   <head>\n",
    "      <title>Dataframe list</title>\n",
    "   </head>\n",
    "   <body>\n",
    "      <h1><font size=\"5\">Part 2: Specialized Dataframes</font></h1>\n",
    "       <p><font size=\"4\">Dataframes for:</font></p>\n",
    "      <ul style=\"list-style-type:none;line-height: 2;\">\n",
    "         <li><font size=\"3\"><a href='#months'>1. Months</a></font></li>\n",
    "         <ul style=\"margin-left: 1em; padding-left: 1em;list-style-type:none\">\n",
    "         <li><font size=\"3\"><a href='#retention'>1.1 Retention Rate and Table</a></font></li>\n",
    "         </ul>          \n",
    "         <li><font size=\"3\"><a href='#users'>2. Users</a></font></li>\n",
    "         <li><font size=\"3\"><a href='#articles'>3. Articles</a></font></li>\n",
    "         <li><font size=\"3\"><a href='#germany'>4. Germany</a></font></li>\n",
    "      </ul>\n",
    "   </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./Data/dataframefull.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='months'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataframe for Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total revenue per month\n",
    "df_month = df.groupby(['year_month'])['total_price'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#women articles per month:\n",
    "df_month = pd.merge(df_month, df.groupby(['year_month'])['women_article'].sum().reset_index(), on=\"year_month\")\n",
    "#men articles per month:\n",
    "df_month = pd.merge(df_month, df.groupby(['year_month'])['men_article'].sum().reset_index(), on=\"year_month\")\n",
    "#kids articles per month:\n",
    "df_month = pd.merge(df_month, df.groupby(['year_month'])['kids_article'].sum().reset_index(), on=\"year_month\")\n",
    "#home and living articles per month:\n",
    "df_month = pd.merge(df_month, df.groupby(['year_month'])['homeandliving_article'].sum().reset_index(), on=\"year_month\")\n",
    "#average shopping cart value per month:\n",
    "df_month = pd.merge(df_month, df.drop_duplicates(subset =\"visit_id\").groupby(['year_month'])['shop_cart_value'].mean().reset_index(), on=\"year_month\")\n",
    "#number of active customers per month:\n",
    "df_month = pd.merge(df_month, df.groupby('year_month')['user_id'].nunique().reset_index(), on=\"year_month\")\n",
    "#number of total purchases per month:\n",
    "df_month = pd.merge(df_month, df.groupby('year_month')['visit_id'].nunique().reset_index(), on=\"year_month\")\n",
    "#average number of items bought per shopping cart:\n",
    "df_month = pd.merge(df_month, df.drop_duplicates(subset =\"visit_id\").groupby(['year_month'])['shop_cart_item_count'].mean().reset_index(), on=\"year_month\")\n",
    "\n",
    "\n",
    "#rename columns to better reflect what they represent\n",
    "df_month = df_month.rename(columns = {\"total_price\": \"revenue\", \"shop_cart_value\": \"avg_shop_cart\", \"visit_id\": \"total_purchases\", \"user_id\": \"active_customers\", \"shop_cart_item_count\": \"avg_cart_itemcount\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add monthly growth of revenue:\n",
    "df_month[\"monthly_growth_revenue\"] = df_month[\"revenue\"].pct_change()\n",
    "#fill the one NaN that comes with the above code:\n",
    "df_month = df_month.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if any column was skipped when reordering (True: all columns fetched): True\n"
     ]
    }
   ],
   "source": [
    "#reordering the columns\n",
    "cols = df_month.columns.tolist()\n",
    "cols = cols[0:2] + [cols[10]] + [cols[6]] + [cols[9]] + cols[7:9] + cols[2:6] \n",
    "print(\"Check if any column was skipped when reordering (True: all columns fetched):\", len(cols) == len(df_month.columns))\n",
    "df_month = df_month.reindex(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revenue for first users\n",
    "df_month = pd.merge(df_month, df[df[\"user_type\"] == \"first\"].groupby(['year_month'])[\"total_price\"].sum().reset_index(), on=\"year_month\")\n",
    "df_month = df_month.rename(columns = {\"total_price\": \"revenue_first\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revenue for established users\n",
    "row = pd.Series({201711: 0},name=0)\n",
    "est_df = df[df[\"user_type\"] == \"established\"].groupby(['year_month'])[\"total_price\"].sum().append(row)\n",
    "est_df = est_df.sort_index()\n",
    "est_df = est_df.reset_index()\n",
    "est_df = est_df.rename(columns = {\"index\": \"year_month\", 0: \"revenue_established\"})\n",
    "df_month = pd.merge(df_month, est_df, on=\"year_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of unique first users each month\n",
    "df_month = pd.merge(df_month, df[df[\"user_type\"] == \"first\"].groupby(['year_month'])[\"user_id\"].nunique().reset_index(), on=\"year_month\")\n",
    "df_month = df_month.rename(columns = {\"user_id\": \"first_user_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count for established users\n",
    "row = pd.Series({201711: 0},name=0)\n",
    "estc_df = df[df[\"user_type\"] == \"established\"].groupby(['year_month'])[\"user_id\"].nunique().append(row)\n",
    "estc_df = estc_df.sort_index()\n",
    "estc_df = estc_df.reset_index()\n",
    "estc_df = estc_df.rename(columns = {\"index\": \"year_month\", 0: \"established_user_count\"})\n",
    "df_month = pd.merge(df_month, estc_df, on=\"year_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revenue per month\n",
    "u_purch = df.groupby(['user_id','year_month'])['total_price'].sum().reset_index()\n",
    "\n",
    "#retention matrix with crosstab\n",
    "retention = pd.crosstab(u_purch['user_id'], u_purch['year_month']).reset_index()\n",
    "\n",
    "#retained and user per month\n",
    "months = retention.columns[:]\n",
    "retention_array = []\n",
    "for i in range(len(months)-1):\n",
    "    retention_data = {}\n",
    "    selected_month = months[i+1]\n",
    "    prev_month = months[i]\n",
    "    retention_data['year_month'] = int(selected_month)\n",
    "    retention_data['user_count'] = retention[selected_month].sum()\n",
    "    retention_data['retained_user_count'] = retention[(retention[selected_month]>0) & (retention[prev_month]>0)][selected_month].sum()\n",
    "    retention_array.append(retention_data)\n",
    "    \n",
    "#array to dataframe and add retention_rate\n",
    "retention = pd.DataFrame(retention_array)\n",
    "retention['retention_rate'] = retention['retained_user_count']/retention['user_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month = pd.merge(df_month, retention.drop(columns = \"user_count\"), on=\"year_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratio = df.query(\"user_type == 'first'\").groupby(['year_month'])['user_id'].nunique()/df.query(\"user_type == 'established'\").groupby(['year_month'])['user_id'].nunique() \n",
    "user_ratio = user_ratio.reset_index()\n",
    "user_ratio = user_ratio.fillna(0)\n",
    "user_ratio = user_ratio.rename(columns={\"user_id\": \"ratio_first_to_established\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month = pd.merge(df_month, user_ratio, on=\"year_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_month</th>\n",
       "      <th>revenue</th>\n",
       "      <th>monthly_growth_revenue</th>\n",
       "      <th>avg_shop_cart</th>\n",
       "      <th>avg_cart_itemcount</th>\n",
       "      <th>active_customers</th>\n",
       "      <th>total_purchases</th>\n",
       "      <th>women_article</th>\n",
       "      <th>men_article</th>\n",
       "      <th>kids_article</th>\n",
       "      <th>homeandliving_article</th>\n",
       "      <th>revenue_first</th>\n",
       "      <th>revenue_established</th>\n",
       "      <th>first_user_count</th>\n",
       "      <th>established_user_count</th>\n",
       "      <th>retained_user_count</th>\n",
       "      <th>retention_rate</th>\n",
       "      <th>ratio_first_to_established</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201711</td>\n",
       "      <td>5.268517e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>103.495016</td>\n",
       "      <td>3.466743</td>\n",
       "      <td>46985</td>\n",
       "      <td>50906</td>\n",
       "      <td>96910</td>\n",
       "      <td>61491</td>\n",
       "      <td>14107</td>\n",
       "      <td>999</td>\n",
       "      <td>5.268517e+06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46985</td>\n",
       "      <td>0</td>\n",
       "      <td>46985</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201712</td>\n",
       "      <td>4.983451e+06</td>\n",
       "      <td>-0.054108</td>\n",
       "      <td>90.652698</td>\n",
       "      <td>3.284649</td>\n",
       "      <td>50004</td>\n",
       "      <td>54973</td>\n",
       "      <td>95202</td>\n",
       "      <td>64962</td>\n",
       "      <td>15461</td>\n",
       "      <td>1806</td>\n",
       "      <td>4.619641e+06</td>\n",
       "      <td>363809.47</td>\n",
       "      <td>46799</td>\n",
       "      <td>3205</td>\n",
       "      <td>3205</td>\n",
       "      <td>0.064095</td>\n",
       "      <td>14.601872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201801</td>\n",
       "      <td>3.717325e+06</td>\n",
       "      <td>-0.254066</td>\n",
       "      <td>87.034361</td>\n",
       "      <td>3.661563</td>\n",
       "      <td>38221</td>\n",
       "      <td>42711</td>\n",
       "      <td>89365</td>\n",
       "      <td>52416</td>\n",
       "      <td>10983</td>\n",
       "      <td>1100</td>\n",
       "      <td>3.216473e+06</td>\n",
       "      <td>500851.97</td>\n",
       "      <td>33596</td>\n",
       "      <td>4625</td>\n",
       "      <td>3141</td>\n",
       "      <td>0.082180</td>\n",
       "      <td>7.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201802</td>\n",
       "      <td>3.056483e+06</td>\n",
       "      <td>-0.177773</td>\n",
       "      <td>90.680675</td>\n",
       "      <td>3.791995</td>\n",
       "      <td>30837</td>\n",
       "      <td>33706</td>\n",
       "      <td>67535</td>\n",
       "      <td>42438</td>\n",
       "      <td>14910</td>\n",
       "      <td>795</td>\n",
       "      <td>2.526812e+06</td>\n",
       "      <td>529670.62</td>\n",
       "      <td>26114</td>\n",
       "      <td>4723</td>\n",
       "      <td>2520</td>\n",
       "      <td>0.081720</td>\n",
       "      <td>5.529113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201803</td>\n",
       "      <td>6.210612e+06</td>\n",
       "      <td>1.031947</td>\n",
       "      <td>108.247849</td>\n",
       "      <td>3.966274</td>\n",
       "      <td>52209</td>\n",
       "      <td>57374</td>\n",
       "      <td>139458</td>\n",
       "      <td>66852</td>\n",
       "      <td>18202</td>\n",
       "      <td>964</td>\n",
       "      <td>5.122223e+06</td>\n",
       "      <td>1088389.01</td>\n",
       "      <td>44512</td>\n",
       "      <td>7697</td>\n",
       "      <td>2797</td>\n",
       "      <td>0.053573</td>\n",
       "      <td>5.783032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_month       revenue  monthly_growth_revenue  avg_shop_cart  \\\n",
       "0      201711  5.268517e+06                0.000000     103.495016   \n",
       "1      201712  4.983451e+06               -0.054108      90.652698   \n",
       "2      201801  3.717325e+06               -0.254066      87.034361   \n",
       "3      201802  3.056483e+06               -0.177773      90.680675   \n",
       "4      201803  6.210612e+06                1.031947     108.247849   \n",
       "\n",
       "   avg_cart_itemcount  active_customers  total_purchases  women_article  \\\n",
       "0            3.466743             46985            50906          96910   \n",
       "1            3.284649             50004            54973          95202   \n",
       "2            3.661563             38221            42711          89365   \n",
       "3            3.791995             30837            33706          67535   \n",
       "4            3.966274             52209            57374         139458   \n",
       "\n",
       "   men_article  kids_article  homeandliving_article  revenue_first  \\\n",
       "0        61491         14107                    999   5.268517e+06   \n",
       "1        64962         15461                   1806   4.619641e+06   \n",
       "2        52416         10983                   1100   3.216473e+06   \n",
       "3        42438         14910                    795   2.526812e+06   \n",
       "4        66852         18202                    964   5.122223e+06   \n",
       "\n",
       "   revenue_established  first_user_count  established_user_count  \\\n",
       "0                 0.00             46985                       0   \n",
       "1            363809.47             46799                    3205   \n",
       "2            500851.97             33596                    4625   \n",
       "3            529670.62             26114                    4723   \n",
       "4           1088389.01             44512                    7697   \n",
       "\n",
       "   retained_user_count  retention_rate  ratio_first_to_established  \n",
       "0                46985        1.000000                    0.000000  \n",
       "1                 3205        0.064095                   14.601872  \n",
       "2                 3141        0.082180                    7.264000  \n",
       "3                 2520        0.081720                    5.529113  \n",
       "4                 2797        0.053573                    5.783032  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month.to_pickle('Data/months.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='retention'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention rate shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**retention rate table** from https://towardsdatascience.com/data-driven-growth-with-python-part-1-know-your-metrics-812781e66a5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revenue per month\n",
    "u_purch = df.groupby(['user_id','year_month'])['total_price'].sum().reset_index()\n",
    "\n",
    "#retention matrix with crosstab\n",
    "retention = pd.crosstab(u_purch['user_id'], u_purch['year_month']).reset_index()\n",
    "\n",
    "#retained and uder per month\n",
    "months = retention.columns[2:]\n",
    "retention_array = []\n",
    "for i in range(len(months)-1):\n",
    "    retention_data = {}\n",
    "    selected_month = months[i+1]\n",
    "    prev_month = months[i]\n",
    "    retention_data['year_month'] = int(selected_month)\n",
    "    retention_data['user_count'] = retention[selected_month].sum()\n",
    "    retention_data['retained_user_count'] = retention[(retention[selected_month]>0) & (retention[prev_month]>0)][selected_month].sum()\n",
    "    retention_array.append(retention_data)\n",
    "    \n",
    "#array to dataframe and add retention_rate\n",
    "retention = pd.DataFrame(retention_array)\n",
    "retention['retention_rate'] = retention['retained_user_count']/retention['user_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_user_count</th>\n",
       "      <th>201712</th>\n",
       "      <th>201801</th>\n",
       "      <th>201802</th>\n",
       "      <th>201803</th>\n",
       "      <th>201804</th>\n",
       "      <th>201805</th>\n",
       "      <th>201806</th>\n",
       "      <th>201807</th>\n",
       "      <th>201808</th>\n",
       "      <th>201809</th>\n",
       "      <th>201810</th>\n",
       "      <th>201811</th>\n",
       "      <th>201812</th>\n",
       "      <th>201901</th>\n",
       "      <th>201902</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year_month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201712</th>\n",
       "      <td>50004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0628</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201801</th>\n",
       "      <td>38221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201802</th>\n",
       "      <td>30837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0907</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201803</th>\n",
       "      <td>52209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201804</th>\n",
       "      <td>44879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201805</th>\n",
       "      <td>54426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0774</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201806</th>\n",
       "      <td>45145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201807</th>\n",
       "      <td>48122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201808</th>\n",
       "      <td>38260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0781</td>\n",
       "      <td>0.0231</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201809</th>\n",
       "      <td>44427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201810</th>\n",
       "      <td>64699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0790</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201811</th>\n",
       "      <td>62561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201812</th>\n",
       "      <td>56140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201901</th>\n",
       "      <td>44993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201902</th>\n",
       "      <td>36190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            total_user_count  201712  201801  201802  201803  201804  201805  \\\n",
       "year_month                                                                     \n",
       "201712                 50004     1.0  0.0628  0.0128  0.0057  0.0026  0.0016   \n",
       "201801                 38221     NaN  1.0000  0.0659  0.0182  0.0062  0.0036   \n",
       "201802                 30837     NaN     NaN  1.0000  0.0907  0.0215  0.0103   \n",
       "201803                 52209     NaN     NaN     NaN  1.0000  0.0727  0.0201   \n",
       "201804                 44879     NaN     NaN     NaN     NaN  1.0000  0.0887   \n",
       "201805                 54426     NaN     NaN     NaN     NaN     NaN  1.0000   \n",
       "201806                 45145     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "201807                 48122     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "201808                 38260     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "201809                 44427     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "201810                 64699     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "201811                 62561     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "201812                 56140     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "201901                 44993     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "201902                 36190     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "            201806  201807  201808  201809  201810  201811  201812  201901  \\\n",
       "year_month                                                                   \n",
       "201712      0.0011  0.0007  0.0005  0.0004  0.0003  0.0003  0.0003  0.0002   \n",
       "201801      0.0023  0.0015  0.0010  0.0007  0.0006  0.0005  0.0005  0.0004   \n",
       "201802      0.0051  0.0031  0.0018  0.0014  0.0010  0.0008  0.0007  0.0006   \n",
       "201803      0.0076  0.0038  0.0021  0.0014  0.0010  0.0008  0.0007  0.0004   \n",
       "201804      0.0199  0.0085  0.0041  0.0025  0.0016  0.0012  0.0010  0.0006   \n",
       "201805      0.0774  0.0215  0.0077  0.0038  0.0024  0.0017  0.0012  0.0008   \n",
       "201806      1.0000  0.0931  0.0220  0.0079  0.0047  0.0029  0.0019  0.0013   \n",
       "201807         NaN  1.0000  0.0794  0.0165  0.0075  0.0041  0.0026  0.0017   \n",
       "201808         NaN     NaN  1.0000  0.0781  0.0231  0.0101  0.0053  0.0032   \n",
       "201809         NaN     NaN     NaN  1.0000  0.1016  0.0256  0.0102  0.0055   \n",
       "201810         NaN     NaN     NaN     NaN  1.0000  0.0790  0.0200  0.0079   \n",
       "201811         NaN     NaN     NaN     NaN     NaN  1.0000  0.0765  0.0189   \n",
       "201812         NaN     NaN     NaN     NaN     NaN     NaN  1.0000  0.0760   \n",
       "201901         NaN     NaN     NaN     NaN     NaN     NaN     NaN  1.0000   \n",
       "201902         NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "            201902  \n",
       "year_month          \n",
       "201712      0.0002  \n",
       "201801      0.0003  \n",
       "201802      0.0005  \n",
       "201803      0.0003  \n",
       "201804      0.0004  \n",
       "201805      0.0006  \n",
       "201806      0.0010  \n",
       "201807      0.0012  \n",
       "201808      0.0021  \n",
       "201809      0.0030  \n",
       "201810      0.0038  \n",
       "201811      0.0072  \n",
       "201812      0.0175  \n",
       "201901      0.0736  \n",
       "201902      1.0000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#create our retentiond table again with crosstab() - we need to change the column names for using them in .query() function\n",
    "retentiondd = pd.crosstab(u_purch['user_id'], u_purch['year_month']).reset_index()\n",
    "new_column_names = [ 'm_' + str(column) for column in retentiondd.columns]\n",
    "retentiondd.columns = new_column_names\n",
    "\n",
    "#create the array of Retained users for each cohort monthly\n",
    "retentiond_array = []\n",
    "for i in range(len(months)):\n",
    "    retentiond_data = {}\n",
    "    selected_month = months[i]\n",
    "    prev_months = months[:i]\n",
    "    next_months = months[i+1:]\n",
    "    for prev_month in prev_months:\n",
    "        retentiond_data[prev_month] = np.nan\n",
    "        \n",
    "    total_user_count =  retentiond_data['total_user_count'] = retentiondd['m_' + str(selected_month)].sum()\n",
    "    retentiond_data[selected_month] = 1 \n",
    "    \n",
    "    plch = \"{} > 0\".format('m_' + str(selected_month))\n",
    "    \n",
    "\n",
    "    for next_month in next_months:\n",
    "        plch = plch + \" and {} > 0\".format(str('m_' + str(next_month)))\n",
    "        retentiond_data[next_month] = np.round(retentiondd.query(plch)['m_' + str(next_month)].sum()/total_user_count,4)\n",
    "    retentiond_array.append(retentiond_data)\n",
    "    \n",
    "retentiondd = pd.DataFrame(retentiond_array)\n",
    "retentiondd.index = months\n",
    "\n",
    "#showing new cohort based retentiond table\n",
    "retentiondd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly this shows that there is close to zero retention happening in the webshopdata. Thus further analysis of retention rate and, also in respect to this, customer churning will not yield any satisfying results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='users'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataframe for Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize dataframe with total revenue per customer\n",
    "df_user = df.groupby('user_id')['total_price'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#women articles the user bought:\n",
    "df_user = pd.merge(df_user, df.groupby(['user_id'])['women_article'].sum().reset_index(), on=\"user_id\")\n",
    "#men articles the user bought:\n",
    "df_user = pd.merge(df_user, df.groupby(['user_id'])['men_article'].sum().reset_index(), on=\"user_id\")\n",
    "#kids articles the user bought:\n",
    "df_user = pd.merge(df_user, df.groupby(['user_id'])['kids_article'].sum().reset_index(), on=\"user_id\")\n",
    "#home and living articles the user bought:\n",
    "df_user = pd.merge(df_user, df.groupby(['user_id'])['homeandliving_article'].sum().reset_index(), on=\"user_id\")\n",
    "#average shopping cart value per customer:\n",
    "df_user = pd.merge(df_user, df.drop_duplicates(subset =\"visit_id\").groupby(['user_id'])['shop_cart_value'].mean().reset_index(), on=\"user_id\")\n",
    "#number of total visits per user:\n",
    "df_user = pd.merge(df_user, df.groupby('user_id')['visit_id'].nunique().reset_index(), on=\"user_id\")\n",
    "#average number of items bought per shopping cart:\n",
    "df_user = pd.merge(df_user, df.drop_duplicates(subset =\"visit_id\").groupby(['user_id'])['shop_cart_item_count'].mean().reset_index(), on=\"user_id\")\n",
    "\n",
    "\n",
    "#rename columns to better reflect what they represent\n",
    "df_user = df_user.rename(columns = {\"total_price\": \"revenue\", \"shop_cart_value\": \"avg_shop_cart\", \"visit_id\": \"total_purchases\", \"shop_cart_item_count\": \"avg_cart_itemcount\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry for date + user for easier use\n",
    "df[\"date_user\"] = df['date'].map(lambda date: 10000*date.year + 100*date.month + date.day).apply(lambda x: str(x)) + df.user_id.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.day_of_week.replace(0, 7, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setday(day, daynr):\n",
    "    \"\"\"\n",
    "    Set the days for users in the df_user dataframe\n",
    "    day - weekday , Monday .... Sunday\n",
    "    daynr - nr of day , here 7,1,2,...,6\n",
    "    \"\"\"\n",
    "    uniq_d = df[df[\"day_of_week\"] == daynr].drop_duplicates(subset = \"date_user\").groupby('user_id')[\"day_of_week\"].sum().reset_index()\n",
    "    uniq_d = uniq_d.set_index(\"user_id\")\n",
    "    del uniq_d.index.name\n",
    "    uniq_calc = (uniq_d//daynr)\n",
    "    uniq_dic = uniq_calc.to_dict()\n",
    "    num_d = df[df[\"day_of_week\"] == daynr].drop_duplicates(subset =\"visit_id\").groupby('user_id')[\"day_of_week\"].sum().reset_index()\n",
    "    num_d = num_d.set_index(\"user_id\")\n",
    "    num_d_calc = ((num_d - uniq_d)//daynr)+1  #articles per day\n",
    "    num_dic = num_d_calc.to_dict()\n",
    "    df_user[str(day)] = df_user[\"user_id\"].map(uniq_dic[\"day_of_week\"])\n",
    "    df_user[\"times_bought_\" + str(day)] = df_user[\"user_id\"].map(num_dic[\"day_of_week\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "daylist = [(\"monday\", 7), (\"tuesday\", 1), (\"wednesday\", 2), (\"thursday\", 3), (\"friday\", 4), (\"saturday\", 5), (\"sunday\", 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in daylist:\n",
    "    setday(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = df_user.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace monday back to 0\n",
    "df.day_of_week.replace(7, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### usertype categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_df = df.groupby('user_id')['user_type'].sum().reset_index()\n",
    "\n",
    "def check_type(item):\n",
    "    return \"first\" if re.search(r\"established\", item) == None else \"established\"\n",
    "\n",
    "type_df['user_type'] = type_df['user_type'].apply(check_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = pd.merge(df_user, type_df, on=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = df.groupby('user_id')[\"country\"].apply(lambda x: ', '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word with the most occurence in a string\n",
    "def freq(x): \n",
    "    out = []\n",
    "    # break the string into list of words \n",
    "    str_list = x.split(sep=\", \") \n",
    "  \n",
    "    # gives set of unique words \n",
    "    unique_words = set(str_list) \n",
    "      \n",
    "    for words in unique_words : \n",
    "        out.append((words, str_list.count(words)))\n",
    "    return max(out,key=itemgetter(1))[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = [\"region\", \"city\", \"device_category\", \"device_browser\", \"device_os\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringbuilder(columnlist):\n",
    "    df_out = df.groupby('user_id')[\"country\"].apply(lambda x: ', '.join(x)).reset_index()\n",
    "    for j in columnlist:\n",
    "        df_out = df_out.join(df.groupby('user_id')[j].apply(lambda x: ', '.join(x)).reset_index().drop(columns=\"user_id\"))\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strings = stringbuilder(list_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_strings = pd.read_pickle('Data/datastrings.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strings = df_strings.drop(columns=[\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_strings.columns:\n",
    "    df_strings[x] = df_strings[x].apply(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slim_country(item):\n",
    "    if item == \"Germany\":\n",
    "        return \"Germany\"\n",
    "    elif item == \"Austria\":\n",
    "        return \"Austria\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "df_strings['country_slim'] = df_strings['country'].apply(slim_country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the string dataframe as the code took quite long to run, so if the notebook needs to rerun i have it\n",
    "df_strings.to_pickle('Data/datastrings.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = df_user.join(df_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user.to_pickle('Data/users.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='articles'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataframe for Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_plchldr = df[[\"visit_id\", \"date\", \"article_id\", \"article_name\", \"price\",\\\n",
    "                  \"women_article\", \"men_article\", \"kids_article\", \"homeandliving_article\" \\\n",
    "                 , \"category_slim\", \"brand\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = art_plchldr.sort_values([\"article_id\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.to_pickle('Data/articles.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='germany'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataframes for Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ger = df[df[\"country\"] == \"Germany\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ger_sorted = df_ger.sort_values([\"region\", \"city\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ger_states = pd.DataFrame(df_ger_sorted.drop_duplicates(subset =\"visit_id\").region.value_counts())\n",
    "df_ger_states.reset_index(inplace=True)\n",
    "df_ger_states = df_ger_states.rename(columns = {\"index\": \"region\", \"region\": \"total_purchases\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#women articles per region:\n",
    "df_ger_states = pd.merge(df_ger_states, df_ger_sorted.groupby(['region'])['women_article'].sum().reset_index(), on=\"region\")\n",
    "#men articles per region:\n",
    "df_ger_states = pd.merge(df_ger_states, df_ger_sorted.groupby(['region'])['men_article'].sum().reset_index(), on=\"region\")\n",
    "#kids articles per region:\n",
    "df_ger_states = pd.merge(df_ger_states, df_ger_sorted.groupby(['region'])['kids_article'].sum().reset_index(), on=\"region\")\n",
    "#home and living articles per region:\n",
    "df_ger_states = pd.merge(df_ger_states, df_ger_sorted.groupby(['region'])['homeandliving_article'].sum().reset_index(), on=\"region\")\n",
    "#average shopping cart value per region:\n",
    "df_ger_states = pd.merge(df_ger_states, df_ger_sorted.drop_duplicates(subset =\"visit_id\").groupby(['region'])['shop_cart_value'].mean().reset_index(), on=\"region\")\n",
    "#number of active customers per region:\n",
    "df_ger_states = pd.merge(df_ger_states, df_ger_sorted.groupby('region')['user_id'].nunique().reset_index(), on=\"region\")\n",
    "#average number of items bought per shopping cart:\n",
    "df_ger_states = pd.merge(df_ger_states, df_ger_sorted.drop_duplicates(subset =\"visit_id\").groupby(['region'])['shop_cart_item_count'].mean().reset_index(), on=\"region\")\n",
    "\n",
    "\n",
    "df_ger_states = df_ger_states.rename(columns = {\"total_price\": \"revenue\", \"shop_cart_value\": \"avg_shop_cart\", \"user_id\": \"active_customers\", \"shop_cart_item_count\": \"avg_cart_itemcount\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_casual(region):\n",
    "    return df_ger_sorted[df_ger_sorted[\"region\"] == str(region)].brand.value_counts().reset_index().brand.loc[0]\n",
    "def set_denim(region):\n",
    "    return df_ger_sorted[df_ger_sorted[\"region\"] == str(region)].brand.value_counts().reset_index().brand.loc[1]                  \n",
    "def set_notset(region):\n",
    "    return df_ger_sorted[df_ger_sorted[\"region\"] == str(region)].brand.value_counts().reset_index().brand.loc[2]                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ger_states[\"casual_brand_count\"] = df_ger_states[\"region\"].apply(set_casual)\n",
    "df_ger_states[\"denim_brand_count\"] = df_ger_states[\"region\"].apply(set_denim)\n",
    "df_ger_states[\"(not_set)_brand_count\"] = df_ger_states[\"region\"].apply(set_notset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ger_states.to_pickle('Data/gerstates.pickle')\n",
    "df_ger_sorted.to_pickle('Data/gersorted.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
